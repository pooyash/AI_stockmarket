{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#random forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Assuming `btc_df` has been preprocessed and has features + 'Candlestick Pattern' column\n",
        "\n",
        "# Feature extraction (e.g., OHLC ratios)\n",
        "btc_df['range'] = btc_df['High'] - btc_df['Low']\n",
        "btc_df['body'] = abs(btc_df['Close'] - btc_df['Open'])\n",
        "features = btc_df[['Open', 'High', 'Low', 'Close', 'Volume', 'range', 'body']]\n",
        "\n",
        "# Encode labels\n",
        "labels = btc_df['Candlestick Pattern'].astype('category').cat.codes\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfpFJkRxoMaP",
        "outputId": "1542b922-ac49-4909-91fb-85f042bbd13c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5298701298701298\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.06      0.09        35\n",
            "           1       0.10      0.02      0.04        41\n",
            "           2       1.00      0.11      0.20        18\n",
            "           3       0.74      0.87      0.80        61\n",
            "           4       0.00      0.00      0.00         7\n",
            "           5       0.27      0.10      0.15        39\n",
            "           6       0.00      0.00      0.00        30\n",
            "           7       0.25      0.06      0.09        36\n",
            "           8       0.00      0.00      0.00        18\n",
            "          10       0.00      0.00      0.00        38\n",
            "          11       0.33      0.09      0.14        11\n",
            "          12       0.00      0.00      0.00        22\n",
            "          13       0.50      0.74      0.60       117\n",
            "          14       0.00      0.00      0.00         1\n",
            "          15       0.00      0.00      0.00         5\n",
            "          16       0.57      0.88      0.69       291\n",
            "\n",
            "    accuracy                           0.53       770\n",
            "   macro avg       0.25      0.18      0.17       770\n",
            "weighted avg       0.42      0.53      0.44       770\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Feature Engineering: Adding Technical Indicators\n",
        "def compute_sma(data, window):\n",
        "    return data.rolling(window=window).mean()\n",
        "\n",
        "def compute_ema(data, window):\n",
        "    return data.ewm(span=window, adjust=False).mean()\n",
        "\n",
        "def compute_rsi(data, window):\n",
        "    delta = data.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
        "    rs = gain / loss\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "# Assuming `btc_df` is the DataFrame with the preprocessed data\n",
        "# Adding additional indicators: SMA, EMA, RSI\n",
        "btc_df['SMA_10'] = compute_sma(btc_df['Close'], 10)\n",
        "btc_df['EMA_10'] = compute_ema(btc_df['Close'], 10)\n",
        "btc_df['RSI'] = compute_rsi(btc_df['Close'], 14)\n",
        "btc_df['range'] = btc_df['High'] - btc_df['Low']\n",
        "btc_df['body'] = abs(btc_df['Close'] - btc_df['Open'])\n",
        "\n",
        "# Prepare the feature set\n",
        "features = btc_df[['Open', 'High', 'Low', 'Close', 'Volume', 'range', 'body', 'SMA_10', 'EMA_10', 'RSI']]\n",
        "\n",
        "# Handle missing values (if any) due to rolling computations\n",
        "features.fillna(method='bfill', inplace=True)\n",
        "\n",
        "# Encode labels\n",
        "labels = btc_df['Candlestick Pattern'].astype('category').cat.codes\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Hyperparameter Tuning with RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Using RandomizedSearchCV to find the best hyperparameters\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf_model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,  # Number of parameter settings sampled\n",
        "    cv=3,  # 3-fold cross-validation\n",
        "    verbose=2,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best model from RandomizedSearchCV\n",
        "best_rf_model = random_search.best_estimator_\n",
        "\n",
        "# Make predictions with the best model\n",
        "y_pred = best_rf_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, labels=np.unique(y_test), target_names=label_encoder.inverse_transform(np.unique(y_test))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kqffx2Cp5pxj",
        "outputId": "b75a032e-a87d-47d2-be56-afbe40efbe11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
            "Best Hyperparameters: {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20}\n",
            "Accuracy: 0.5493506493506494\n",
            "Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "   BEARISH ENGULFING       1.00      0.03      0.06        35\n",
            "   BULLISH ENGULFING       0.00      0.00      0.00        41\n",
            "    DARK CLOUD COVER       0.00      0.00      0.00        18\n",
            "                DOJI       0.73      0.84      0.78        61\n",
            "   EVENING DOJI STAR       0.00      0.00      0.00         7\n",
            "        EVENING STAR       0.00      0.00      0.00        39\n",
            "              HAMMER       0.00      0.00      0.00        30\n",
            "         HANGING MAN       0.00      0.00      0.00        36\n",
            "     INVERTED HAMMER       0.00      0.00      0.00        18\n",
            "        MORNING STAR       0.00      0.00      0.00        38\n",
            "       PIERCING LINE       0.00      0.00      0.00        11\n",
            "       SHOOTING STAR       0.00      0.00      0.00        22\n",
            "        SPINNING TOP       0.49      0.79      0.61       117\n",
            "   THREE BLACK CROWS       0.00      0.00      0.00         1\n",
            "THREE WHITE SOLDIERS       0.00      0.00      0.00         5\n",
            "             Unknown       0.56      0.96      0.70       291\n",
            "\n",
            "            accuracy                           0.55       770\n",
            "           macro avg       0.17      0.16      0.13       770\n",
            "        weighted avg       0.39      0.55      0.42       770\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lstm+cnn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Assume btc_df is the DataFrame with the labeled candlestick patterns\n",
        "# Prepare the features and labels for the model\n",
        "btc_df['range'] = btc_df['High'] - btc_df['Low']\n",
        "btc_df['body'] = abs(btc_df['Close'] - btc_df['Open'])\n",
        "features = btc_df[['Open', 'High', 'Low', 'Close', 'Volume', 'range', 'body']]\n",
        "\n",
        "# Encode labels for classification\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(btc_df['Candlestick Pattern'])\n",
        "\n",
        "# Prepare sequences for CNN-LSTM (e.g., 10-candle sequences)\n",
        "sequence_length = 10\n",
        "\n",
        "# Helper function to create sequences\n",
        "def create_sequences(features, labels, sequence_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(features) - sequence_length):\n",
        "        X.append(features[i:i + sequence_length])\n",
        "        y.append(labels[i + sequence_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Create sequences for training\n",
        "X, y = create_sequences(features_scaled, labels, sequence_length)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 1: Build the CNN-LSTM Model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(sequence_length, X.shape[2])))\n",
        "model.add(layers.MaxPooling1D(pool_size=2))\n",
        "model.add(layers.LSTM(50, return_sequences=False))\n",
        "model.add(layers.Dense(50, activation='relu'))\n",
        "model.add(layers.Dense(len(np.unique(labels)), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 2: Train the CNN-LSTM Model\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=16, validation_split=0.2)\n",
        "\n",
        "# Step 3: Make Predictions and Evaluate the Model\n",
        "\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "# Get unique classes in y_test to ensure the report contains only relevant classes\n",
        "unique_classes_in_test = np.unique(y_test)\n",
        "\n",
        "# Step 4: Evaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, labels=unique_classes_in_test, target_names=label_encoder.inverse_transform(unique_classes_in_test)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsSInK6HqLHN",
        "outputId": "c7d62f19-32f3-4a5c-d65c-6c56870080fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3689 - loss: 2.5580 - val_accuracy: 0.3715 - val_loss: 2.2067\n",
            "Epoch 2/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.3853 - loss: 2.1525 - val_accuracy: 0.3715 - val_loss: 2.1815\n",
            "Epoch 3/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.3990 - loss: 2.0382 - val_accuracy: 0.3715 - val_loss: 2.1646\n",
            "Epoch 4/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3826 - loss: 2.1101 - val_accuracy: 0.3715 - val_loss: 2.1640\n",
            "Epoch 5/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4013 - loss: 2.0676 - val_accuracy: 0.3715 - val_loss: 2.1660\n",
            "Epoch 6/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4000 - loss: 2.0976 - val_accuracy: 0.3715 - val_loss: 2.1669\n",
            "Epoch 7/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3888 - loss: 2.0795 - val_accuracy: 0.3715 - val_loss: 2.1571\n",
            "Epoch 8/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3716 - loss: 2.1238 - val_accuracy: 0.3715 - val_loss: 2.1631\n",
            "Epoch 9/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3828 - loss: 2.0886 - val_accuracy: 0.3715 - val_loss: 2.1580\n",
            "Epoch 10/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4068 - loss: 2.0733 - val_accuracy: 0.3715 - val_loss: 2.1619\n",
            "Epoch 11/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3970 - loss: 2.0725 - val_accuracy: 0.3715 - val_loss: 2.1660\n",
            "Epoch 12/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3852 - loss: 2.0571 - val_accuracy: 0.3715 - val_loss: 2.1537\n",
            "Epoch 13/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3717 - loss: 2.1046 - val_accuracy: 0.3715 - val_loss: 2.1698\n",
            "Epoch 14/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3862 - loss: 2.0888 - val_accuracy: 0.3715 - val_loss: 2.1583\n",
            "Epoch 15/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3911 - loss: 2.0962 - val_accuracy: 0.3715 - val_loss: 2.1588\n",
            "Epoch 16/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3938 - loss: 2.0735 - val_accuracy: 0.3715 - val_loss: 2.1616\n",
            "Epoch 17/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3941 - loss: 2.0776 - val_accuracy: 0.3715 - val_loss: 2.1643\n",
            "Epoch 18/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3878 - loss: 2.0786 - val_accuracy: 0.3715 - val_loss: 2.1491\n",
            "Epoch 19/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3800 - loss: 2.0912 - val_accuracy: 0.3715 - val_loss: 2.1466\n",
            "Epoch 20/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3924 - loss: 2.0615 - val_accuracy: 0.3715 - val_loss: 2.1479\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
            "Accuracy: 0.39504563233376794\n",
            "Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "   BEARISH ENGULFING       0.00      0.00      0.00        46\n",
            "   BULLISH ENGULFING       0.00      0.00      0.00        34\n",
            "    DARK CLOUD COVER       0.00      0.00      0.00        12\n",
            "                DOJI       0.00      0.00      0.00        63\n",
            "   EVENING DOJI STAR       0.00      0.00      0.00         7\n",
            "        EVENING STAR       0.00      0.00      0.00        39\n",
            "              HAMMER       0.00      0.00      0.00        27\n",
            "         HANGING MAN       0.00      0.00      0.00        31\n",
            "     INVERTED HAMMER       0.00      0.00      0.00        17\n",
            "   MORNING DOJI STAR       0.00      0.00      0.00         2\n",
            "        MORNING STAR       0.00      0.00      0.00        24\n",
            "       PIERCING LINE       0.00      0.00      0.00        10\n",
            "       SHOOTING STAR       0.00      0.00      0.00        30\n",
            "        SPINNING TOP       0.00      0.00      0.00       114\n",
            "THREE WHITE SOLDIERS       0.00      0.00      0.00         8\n",
            "             Unknown       0.40      1.00      0.57       303\n",
            "\n",
            "            accuracy                           0.40       767\n",
            "           macro avg       0.02      0.06      0.04       767\n",
            "        weighted avg       0.16      0.40      0.22       767\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-tuner\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFwVppvGxLlJ",
        "outputId": "f6200842-1209-46b5-dbd6-6b982d15653f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#autoencoder+MLP\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Assume btc_df is the DataFrame with the labeled candlestick patterns\n",
        "# Prepare the features and labels for the model\n",
        "btc_df['range'] = btc_df['High'] - btc_df['Low']\n",
        "btc_df['body'] = abs(btc_df['Close'] - btc_df['Open'])\n",
        "features = btc_df[['Open', 'High', 'Low', 'Close', 'Volume', 'range', 'body']]\n",
        "\n",
        "# Encode labels for classification\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(btc_df['Candlestick Pattern'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 1: Build the Autoencoder for Feature Extraction\n",
        "input_dim = X_train_scaled.shape[1]  # Number of features\n",
        "encoding_dim = 5  # Compressing to 5 features\n",
        "\n",
        "# Define the autoencoder model\n",
        "input_layer = layers.Input(shape=(input_dim,))\n",
        "encoded = layers.Dense(encoding_dim, activation='relu')(input_layer)\n",
        "decoded = layers.Dense(input_dim, activation='sigmoid')(encoded)\n",
        "\n",
        "# Compile the autoencoder\n",
        "autoencoder = models.Model(input_layer, decoded)\n",
        "encoder = models.Model(input_layer, encoded)  # For feature extraction\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the autoencoder\n",
        "autoencoder.fit(X_train_scaled, X_train_scaled, epochs=50, batch_size=16, shuffle=True, validation_data=(X_test_scaled, X_test_scaled))\n",
        "\n",
        "# Step 2: Extract Features Using the Encoder\n",
        "X_train_encoded = encoder.predict(X_train_scaled)\n",
        "X_test_encoded = encoder.predict(X_test_scaled)\n",
        "\n",
        "# Step 3: Use MLP Classifier with Encoded Features\n",
        "mlp_classifier = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=200, random_state=42)\n",
        "mlp_classifier.fit(X_train_encoded, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = mlp_classifier.predict(X_test_encoded)\n",
        "\n",
        "# Get the unique classes in y_test to match the labels in the classification report\n",
        "unique_classes_in_test = np.unique(y_test)\n",
        "\n",
        "# Step 4: Evaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=label_encoder.inverse_transform(unique_classes_in_test), labels=unique_classes_in_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJBhds-eo-dk",
        "outputId": "6a443f26-d153-4075-bcdb-8c8e8f405620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1202 - val_loss: 0.0966\n",
            "Epoch 2/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0913 - val_loss: 0.0749\n",
            "Epoch 3/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0710 - val_loss: 0.0570\n",
            "Epoch 4/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0544 - val_loss: 0.0411\n",
            "Epoch 5/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0392 - val_loss: 0.0284\n",
            "Epoch 6/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0266 - val_loss: 0.0193\n",
            "Epoch 7/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0185 - val_loss: 0.0132\n",
            "Epoch 8/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0123 - val_loss: 0.0095\n",
            "Epoch 9/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0093 - val_loss: 0.0072\n",
            "Epoch 10/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0070 - val_loss: 0.0058\n",
            "Epoch 11/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0054 - val_loss: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0052 - val_loss: 0.0045\n",
            "Epoch 13/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0045 - val_loss: 0.0042\n",
            "Epoch 14/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0041 - val_loss: 0.0039\n",
            "Epoch 15/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0040 - val_loss: 0.0034\n",
            "Epoch 16/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0033 - val_loss: 0.0030\n",
            "Epoch 17/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0030 - val_loss: 0.0027\n",
            "Epoch 18/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0027 - val_loss: 0.0025\n",
            "Epoch 19/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0025 - val_loss: 0.0023\n",
            "Epoch 20/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0023 - val_loss: 0.0022\n",
            "Epoch 21/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0022 - val_loss: 0.0021\n",
            "Epoch 22/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
            "Epoch 23/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0020 - val_loss: 0.0019\n",
            "Epoch 24/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0019\n",
            "Epoch 25/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0019 - val_loss: 0.0018\n",
            "Epoch 26/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 27/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 28/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0016 - val_loss: 0.0017\n",
            "Epoch 29/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 30/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 31/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 32/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 33/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 34/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 35/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 36/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 37/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 38/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 39/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 40/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 41/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 42/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 43/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 44/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 45/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 46/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 47/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 48/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 49/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 50/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Accuracy: 0.38571428571428573\n",
            "Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "   BEARISH ENGULFING       0.00      0.00      0.00        35\n",
            "   BULLISH ENGULFING       0.00      0.00      0.00        41\n",
            "    DARK CLOUD COVER       0.00      0.00      0.00        18\n",
            "                DOJI       0.00      0.00      0.00        61\n",
            "   EVENING DOJI STAR       0.00      0.00      0.00         7\n",
            "        EVENING STAR       0.00      0.00      0.00        39\n",
            "              HAMMER       0.00      0.00      0.00        30\n",
            "         HANGING MAN       0.00      0.00      0.00        36\n",
            "     INVERTED HAMMER       0.00      0.00      0.00        18\n",
            "        MORNING STAR       0.00      0.00      0.00        38\n",
            "       PIERCING LINE       0.00      0.00      0.00        11\n",
            "       SHOOTING STAR       0.00      0.00      0.00        22\n",
            "        SPINNING TOP       0.27      0.32      0.29       117\n",
            "   THREE BLACK CROWS       0.00      0.00      0.00         1\n",
            "THREE WHITE SOLDIERS       0.00      0.00      0.00         5\n",
            "             Unknown       0.41      0.89      0.56       291\n",
            "\n",
            "            accuracy                           0.39       770\n",
            "           macro avg       0.04      0.08      0.05       770\n",
            "        weighted avg       0.20      0.39      0.26       770\n",
            "\n"
          ]
        }
      ]
    }
  ]
}